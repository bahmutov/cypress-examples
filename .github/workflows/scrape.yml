# Scrape deployed site into Algolia index
# https://glebbahmutov.com/blog/scrape-static-site-with-algolia/
name: scrape
on:
  # allow running the scraper manually
  workflow_dispatch:
  # and scrape weekly
  schedule:
    - cron: '0 0 * * TUE'

jobs:
  scrape:
    runs-on: ubuntu-20.04
    steps:
      # we need our scraping config, thus check out code
      - name: Checkout ğŸ›ï¸
        uses: actions/checkout@v2
        with:
          persist-credentials: false

      - name: Install dependencies ğŸ“¦
        uses: cypress-io/github-action@v3
        with:
          # just perform install
          runTests: false

      - name: Set Cypress version ğŸ
        id: cypress
        run: echo "::set-output name=version::$(node ./src/print-cypress-version)"

      - name: Print Cypress version ğŸ–¨
        run: echo "Cypress version is ${{ steps.cypress.outputs.version }}"

      - name: Update Algolia config ğŸ“
        run: node ./update-algolia-config https://glebbahmutov.com/cypress-examples/

      # when scraping the site, inject secrets as environment variables
      # then pass their values into the Docker container using "-e" syntax
      # and inject config.json contents as another variable
      # You can run the scraper locally using as-a to pass the environment variables
      # and using a local Docker container
      # $ as-a . docker run-e APPLICATION_ID -e API_KEY ...
      - name: scrape the site ğŸ§½
        env:
          APPLICATION_ID: ${{ secrets.ALGOLIA_APPLICATION_ID }}
          API_KEY: ${{ secrets.ALGOLIA_API_KEY }}
        run: |
          docker run \
          -e APPLICATION_ID -e API_KEY \
          -e CONFIG="$(cat algolia-config.json)" \
          algolia/docsearch-scraper:v1.6.0
